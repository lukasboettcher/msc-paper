\chapter{PTAGPU} \label{chap:main}
This thesis presents a software library named PTAGPU, the name is derived from pointer analysis (PTA) and graphics processing unit (GPU).
As the name suggests the core idea is to use GPUs for the purpose of performing a pointer analysis.
The library PTAGPU was developed as a whole program analysis module inside the SVF framework which is in turn built on top of the LLVM compiler system.
\section{Integrating PTAGPU into SVF}\label{sec:integsvf}
As described in \autoref{sec:svf} the SVF framework is capable of processing the LLVM-IR of a compiled program and capture the individual LLVM-IR instructions in a program assignment graph.
When SVF is launched for a whole program analysis, the program assignment graph is further processed into a constraint graph that holds holds all relevant constraints for an initial pointer analysis, see \autoref{tab:ander}.
At this point the constraint graph is passed into a class that inherits from the PointerAnalysis class, see \autoref{fig:pta-svf} for an overview of the pointer analysis class hierarchy in SVF.
Since our goal is to implement a custom pointer analysis, we can inject out own implementation at this stage as a PointerAnalysis subclass.
Specifically we inherit from the Andersen class which is itself a subclass of the PointerAnalysis class that implements the Andersen inclusion-based pointer analysis algorithm in SVF.
By extensive use of runtime polymorphism most of SVF is implemented via virtual member functions - a construct specific to c++ - allowing for function overriding in subclasses. This makes implementing a custom pointer analysis easy as we can reuse most of the initialization steps and program assignment graph processing from the superclasses. Part of the processing is an initial topological sorting and the previously mentioned interpretation into a constraint graph.
As a result we end up with a constraint graph and all strongly connected components of said graph when we initialize our PTAGPU class that inherits from the Andersen class.
The in-memory representation of constraint graphs or generic graphs in SVF is more akin to a linked list data structure where each node carries references to all outgoing and incoming edges and those edges carry references to source and destination nodes as well as auxiliary information, which is an ideal memory model for iterative algorithms such as the default Andersen algorithm where the algorithm works from one node to the next.
Unfortunately this memory model is not ideal for parallel processing.
For this reason we initially reinterpret the constraint graph into a more fitting data structure.
While iterating through the entire constraint graph, we differentiate by edge types and collect all $(src,dst)$ edge pairs in standard library vectors.
If we refer back to the design principles of SVF in \autoref{sec:svf}, where pointer analyses were conceptually split into three components, the Rules, the Graph and the Solver, we effectively implement our own Graph component by using a different linear memory representation for the constraint graph. 
The underlying goal of putting the constraint graph into a linear in-memory data structure is to allow us to more easily copy the memory region containing the relevant information into GPU device memory, which is where our pointer analysis will operate on the data.
Similarly to the Graph component we also modify the Solver and Rule components in the custom analysis implementation.
The details of the implementation will be described in detail in \autoref{sec:design}.

\section{Goal of the Algorithm}
Our goal is to use the provided program assigment graph from SVF and the derived constraint graph to compute a points-to set for each pointer variable in the program.
Overall our algorithm is supposed to serve as an initial pointer analysis pass in the SVF framework. Together with the pointer information we can then proceed to build an over approximated call graph.
One might assume that no pointer information is needed for SVF to build a call graph for the program. Unfortunately indirect function invocations, where function are accessed by pointer dereferencing itself, requires us to build pointer information in order to create an over approximated sound call graph. The over approximate nature stems from the imprecision of Andersen's analysis. This limitation always remains, no matter the algorithm, since pointer analyses are fundamentally undecidable as was mentioned in \autoref{sec:pta}.
It is important to consider that the initial pointer analysis does not directly produce any valuable information in terms of static analysis purposes. Instead we use the pointer information produced by the initial analysis for the call graph which is then used to refine the analysis result by applying a more precise flow- and/or context-sensitive analysis that produces the relevant results, which can then be used to derive value flow information that can directly be used for actual analysis purposes.
Since we are applying a more precise pointer analysis at a later stage anyways one might argue that it would be wise to start off with such an analysis. For small programs this is a viable strategy, unfortunately these more precise analyses are currently not scalable for a whole program analysis and are only applied on-demand \cite{sui2016svf}. This is also one of the motivations for trying to accelerate the initial Anderen analysis specifically since it is performed on the entire program and thus can in theory profit from the paralellism of GPUs.

\section{Design of the Algorithm}\label{sec:design}
The PTAGPU library uses CUDA\footnote{\url{https://developer.nvidia.com/cuda-toolkit}}, an application programming interface language from NVIDIA, to program GPUs in C++.
CUDA accomodates developers with a collection of abstractions that simplify operations with GPU compute and memory. NVIDIA also provides a standalone library for common parallel oeprations such as sorting and transformations named Thrust\footnote{\url{https://docs.nvidia.com/cuda/thrust/index.html}} which is also employed by PTAGPU for common sorting and deduplication operations.
In principle all calculations that are executed with a GPU are denoted as kernels in CUDA. While GPU kernels and CPU code can be shared, CUDA provides some intrinsic operations that can only function when executed on a GPU.
Likewise all CPU operations that rely on external libraries or non standard-library code are not supported in GPU kernels.

\subsection{CUDA Architecture}
The CUDA programming model revolves around blocks of threads. Each block of a kernel executes the same code with the same number of threads.
Thread blocks are further divided into Warps, which are a collection of 32 threads each. This type of parallel processing is called SIMT, for single instruction, multiple threads.
In reality a Warp is more analogous to a single vectorized operation that executes 32 units of work or lanes at once than a collection of individual threads, although each thread in a Warp has its own instruction address counter. Having instruction address counters per thread allows independent branching, resulting in thread divergence. The divergence is implemented by executing each conditional branch sequentially for all threads in a Warp and disabling the threads that do not execute the specific branch.
This way each thread in a Warp always executes the same instruction if active. Since branches are not executed in parallel, thread divergence is discouraged if possible for performance reasons.
These specifications of Warps are uniform in all CUDA hardware and resemble a single group of work that can be scheduled on the device specific number of multiprocessors.
To more easily differentiate between different generations of hardware, the CUDA programming model is segmented into tiers of compute capabilities, where newer hardware with newer capabilities receives a higher compute capability.
Each CUDA capable device has a number of streaming multiprocessors, in short SMs, which themselves have a certain amount of L1 shared memory and number of registers per core among other resources. 
While this is similar to how CPU cores operate, GPU programming uses a flat memory hierarchy with less reliance on caching and more on raw memory bandwidth. For this reason each core in an SM has a relatively large register file so that a single CUDA thread commonly uses hundreds of registers.
The entire work that is to be performed by a single kernel is called a grid, which is divided evenly into blocks, which are divided into Warps. Both the grid and each block can be indexed in up to three dimensions, which is useful for working with shader, global illumination rendering and less useful for static analyses.
To start a computation, a collection of thread blocks are assigned to the available SMs of a GPU. Depending on the hardware and compute capability, a single SM can execute multiple Warps from the assigned thread blocks in parallel as long as enough resources are available in the SM.
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{img/a100.png}
    \caption[Diagram of a single A100 SM]{A single SM of an A100 GPU.\\Taken from an NVIDIA Blog Post\footnotemark}
    \label{fig:cuda-sm}
\end{figure}
\footnotetext{\url{https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}}
The execution order of individual Warps is handled by a Warp scheduler, that decides what Warps get executed at what time.
The purpose of overprovisioning SMs with more blocks/Warps than they can concurrently execute is that when a single Warp executes a memory read/write operation other Warps can execute while the device is busy fetching the data.
Key specification for each SM of a certain compute capability are the following:
\begin{itemize}
    \item Memory Bandwith per SM
    \item Total shared memory per SM
    \item Max number of threads per SM
    \item Max number of blocks per SM
    \item Total number of registers of all cores in SM
\end{itemize}

\subsubsection{Occupancy}
Since each SM has multiple limited resources that can be controlled by the developer, namely register count, shared memory and thread count, a kernel has to be designed with these limitations in mind such that a maximally concurrent execution is possible.
Given a device with compute capability 8.0 and a kernel that requires 256 registers per thread while also running 256 threads per block, we would essentially limit our execution to a single block per SM, since a device with compute capability 8.0 has 64K registers per SM.
This might be disadvantageous, since the SM is in theory capable of working on up to 1024 threads.
We might profit from reducing the amount of registers used per thread and thus increasing the occupancy on our GPU.
Counterintuitively high occupancy does not always lead to better performance. Some programs disproportionally profit from the use of a single resource.
For example compute intensive kernels require more registers per thread than kernels that are heavily reliant on memory operations. Increasing the use of shared memory does not always lead to a performance improvement and decreasing the number of required registers is not always possible.
\subsubsection{Memory Accesses in CUDA}
While modern GPUs can in theory perform multiple TFLOPS of calculations per second, effectively all calculations are limited by memory bandwidth.
Consequently, how we access the GPU memory is very important for the overall performance of our analysis. Specifically coalesced memory accesses of individual Warps where consecutive threads access consecutive memory addresses are important so the memory read operation can be performed within a single transaction and not multiple strided reads.
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{img/cuda.png}
    \caption{Diagram of the CUDA memory architecture for an A100 GPU.\\Taken from \cite{cudarefresher}.}
    \label{fig:cuda-arch}
\end{figure}
The GPU memory controller on modern graphics cards can typically execute memory read or write operations in granularities of 32 bytes up to 128 bytes in total.
As a result it is for example more efficient to load a single byte of memory per thread in a Warp, compared to loading 8 bytes per thread, since we would exceed the maximum of 128 bytes per memory transaction and require multiple memory accesses.
In terms of time needed per instruction, global memory operations are typically two orders of magnitude slower compared to operations on SM registers or shared memory in L1 cache. For this reason CUDA programs can perform exponentially worse if memory accesses are random or strided inefficiently instead of coalesced.

\subsubsection{Unified Memory}
When CUDA code is executed on a 64-bit host system, the developer can use a single memory address space for host and device memory.
Using this unified memory address space allows memory access from both CPU and GPU in the same address space without explicit memory copy operations.
The memory in question is moved implicitly to the device that performs the read operation. The CUDA API also allows the developer to assign preferred residency for specific memory regions, as well as prefetching memory asynchronously for a device. When prefetching is properly employed, unified memory achieves the same performance as dedicated device memory \cite{cudaunifiedmem}.
The major advantage of unified memory is the fact that the memory allocation size is only limited by system memory, not device memory.
This allows a CUDA program to potentially hold vastly more data in memory than would be possible with only GPU memory, while also moving the needed memory regions without much involvement of the developer.
Crucially this keeps large data structures intact without splitting and partitioning them for incremental loading into GPU memory.
This mirrors some of the core ideas of Graspan \autoref{sec:graspan} where parts of the graph are written to disk to conserve memory usage.
Since many high performance computing environments have vastly more main memory available than individual GPUs have device memory, this enables us to expand the scope of our analysis without requiring specialized GPU hardware.

\subsubsection{CUDA Streams}
GPUs are massively parallel. Oftentimes developers do want to perform multiple computations encapsulated in kernels in parallel.
These computations require memory operations before and after to load and store the data to operate on.
Since most GPUs allow for concurrent memory and compute operations it is desirable to be able to efficiently schedule concurrent operations such that while one kernel executes, another performs memory operations.
Because kernels are launched asynchronously, we can utilize CUDA streams to string together multiple compute and memory operations so they are executed in order without interfering with other streams.
The GPU is then able to efficiently schedule multiple streams in order to maximize GPU utilization, see  \autoref{fig:cuda-stream} for an illustration.
By default all cuda operations are executed on the default stream and thus memory operations block each other.
Streams have to be created by the developer and are represented by structs in the program. These elements are then passed as arguments to the CUDA api function calls.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{img/cuda-stream.png}
    \caption[CUDA strean illustration]{CUDA Stream: Serial Model vs Concurrent Model by Lei Mao Å‚icensed under \href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0}; an illustration for the advantages of the CUDA stream api.}
    \label{fig:cuda-stream}
\end{figure}

\subsection{Initialization of CUDA code}\label{sec:init}
As soon as the constraint graph is handed over to the CUDA section of the code some global state is initialized with the information from the constraint graph.
This includes various counter variables for the number of nodes in the graph, as well as the initial unified memory allocations.
When the counter for the number of nodes is first initialized the node count is read from the constraint graph and increased by 20 percent to reserve headroom for new nodes that might be added to the constraint graph during the execution of the algorithm.
This ensures that each node has a well defined place in memory that can not be overwritten by subsequent nodes.
In the current implementation, the algorithm allocates a fixed amount of unified memory accessible from the CPU and GPU. While this requires overprovisioning of memory for a given analysis without knowing the exact amount needed for the pointer analysis, dynamic allocation proves to be very challenging as it is difficult to deduce the exact required amount of GPU memory from a given constraint graph for a pointer analysis.
Furthermore the allocated unified memory is further split into partitions of fixed size for the individual relations of the Andersen analysis.
This includes three memory regions for pointer relations, one for copy relations and two regions for load and store relations between nodes.
See the following diagram for the memory layout of an example allocation of 32 GiB of unified memory.
\begin{center}
    \begin{bytefield}{32}
        \begin{rightwordgroup}{includes all pointer relations\\that have been computed\\at any stage}
            \memsection{ffff ffff}{a000 0000}{12}{pointer constraints}
        \end{rightwordgroup}\\
        \begin{rightwordgroup}{these memory regions are used\\to compute the delta updates\\for the pointer relations}
            \memsection{9fff ffff}{6000 0000}{8}{current pointer constraints}\\
            \memsection{5fff ffff}{2000 0000}{8}{next pointer constraints}
        \end{rightwordgroup}\\
        \begin{rightwordgroup}{static memory\\does not change once written}
            \memsection{1fff ffff}{1000 0000}{2}{load constraints}\\
            \memsection{0fff ffff}{0000 0000}{2}{store constraints}
        \end{rightwordgroup}\\
    \end{bytefield}
\end{center}
Since a pointer analysis in general does not create new load or store relations in the constraint graph, these memory regions do not increase in size after initialization.
Notably the pointer relations are split into three separate memory regions. This is needed for diff-points calculations. This specific optimization was inspired by \cite{mendez2010parallel} and will be explained in detail in \autoref{sec:memory}.
After memory allocation is completed, CUDA streams are initialized for concurrent write operations into each memory region, as well as one stream for each of the available devices to enable computation on multiple GPUs in parallel.
Finally the entire allocation of unified memory is set to ones by \verb|cudaMemset|. This ensures that the memory is not in an undefined state. Furthermore we can utilize this when reading from memory, since a memory region with all bits set to one represents unused space.

% This state is needed for the 

% initializing cuda V var, overhead for new nodes

% subsec unified memory explain

% subsec explain sparse bv layout

% go through idea of inv edges for concurrent rewriting

% based on \cite{mendez2010parallel} and \cite{mendez2012gpu}



% \begin{center}
%     \begin{bytefield}{32}
%         \bitheader{0-31} \\
%         \bitbox{4}{Four} & \bitbox{8}{Eight} &
%         \bitbox{16}{Sixteen} & \bitbox{4}{Four}
%     \end{bytefield}
% \end{center}
% \blindtext[1]
% \begin{center}
%     \definecolor{lightcyan}{rgb}{0.84,1,1}
%     \definecolor{lightgreen}{rgb}{0.64,1,0.71}
%     \definecolor{lightred}{rgb}{1,0.7,0.71}
%     \begin{bytefield}[bitheight=\widthof{~Base~},
%             boxformatting={\centering\small},rightcurly=., rightcurlyspace=0pt]{32}
%         % \bitlabel{29}{Data} & \bitlabel{1}{Base} & \bitlabel{2}{Next} \\
%         \bitheader{0,28,29,30,31} \\
%         \begin{rightwordgroup}{
%                 \begin{tikzpicture}
%                     \tikzstyle{node}=[circle, draw=blue!50, fill=blue!20, inner sep=1pt, minimum size=6mm]
%                     \tikzstyle{linenode}=[pos=0.5,fill=white,inner sep=2pt,outer sep=2pt]
%                     \node[node] (A) at (0,0) {\small$\%1$};
%                     \node[node] (B) at (2,0) {\small$\%2$};
%                     \path [->] (A) edge[bend left] node[linenode] {$p$} (B);
%                 \end{tikzpicture}
%             }
%             \bitbox{29}[bgcolor=lightred]{Data} &
%             \bitbox{1}[bgcolor=lightcyan]{\rotatebox{90}{Base}} &
%             \bitbox{2}[bgcolor=lightgreen]{\rotatebox{90}{Next}}
%         \end{rightwordgroup}
%     \end{bytefield}
% \end{center}

\subsection{Sparse Bitvectors}
At the core of PTAGPU all edges of the constraint graph are stored in sparse bitvectors.
Sparse bitvectors are a data structure that can be used for storing binary values, such as adjacency information of a graph. Sparse bitvectors are especially suited to represent edges of very sparse graphs and allow for dynamic addition of new edges and nodes, something other sparse graph representation such as the compressed sparse row format do not facilitate. Since the constraint graph of pointer analysis problems is typically very sparse \cite{mendez2012gpu}, sparse bitvectors are an ideal data structure for pointer analysis on the GPU.
Individual bitvectors contain three separate component, a base value, a set of bits and a next pointer.
For the bitvectors to efficiently work, the entire codomain of the directed edge relation of a graph is split into evenly sized partitions.
The edges of each partition, if it contains edges, are then stored in the bits of a bitvector. If we omit empty partitions of the edge codomain, we create a set of sparse bitvectors.
The base value of each sparse bitvector represent the current offset for all bits in the bitvector.
Similar to linked lists, sparse bitvectors reference the next sparse bitvector in a field. This allows iteration through adjacent outgoing edges for a given node by working through the next bitvectors until there is no next bitvector defined.
The head bitvector for each node is stored in a pre defined memory location, which is directly derivable from the node index.
The exact position for each node in each memory partition can be calculated by multiplying the node index with the width of a sparse bitvector. This offset is then added to the overall offset of the memory partition to find the head bitvector for a specific edge type, see \autoref{lst:getindex} for the implementation in PTAGPU.
\begin{listing}
    \begin{minted}{c++}
// src it the node for which we want to find the head bitvector
// rel is the andersen relation for which we want to find the outgoing edges
__host__ __device__ index_t getIndex(uint src, uint rel)
{
    switch (rel)
    {
    case PTS:
        return OFFSET_PTS + (ELEMENT_WIDTH * src);
    case PTS_CURR:
        return OFFSET_PTS_CURR + (ELEMENT_WIDTH * src);
    case PTS_NEXT:
        return OFFSET_PTS_NEXT + (ELEMENT_WIDTH * src);
    case COPY:
        return OFFSET_COPY + (ELEMENT_WIDTH * src);
    case LOAD:
        return OFFSET_LOAD + (ELEMENT_WIDTH * src);
    case STORE:
        return OFFSET_STORE + (ELEMENT_WIDTH * src);
    }
    return src * ELEMENT_WIDTH;
}
    \end{minted}
    \caption{Calculating the correct index of a node's head bitvector in unified memory.}
    \label{lst:getindex}
\end{listing}
Following, each unit in the sparse bitvector linked list will be called an element as is conventional for linked lists.
When deciding on the memory characteristics of the individual elements, the traits of the CUDA api need to be taken into consideration.
One possible optimization are coalesced memory accesses, where consecutive threads in a Warp access consecutive memory locations.
Furthermore we can optimize the memory operations of a Warp such that we access 128 bytes in total per Warp, since this allows us to fully saturates the memory controller while only requiring a single coalesced memory transaction \cite{mendez2012gpu}.
If we take the optimal memory transaction of 128 bytes and split it evenly across all 32 threads of a Warp, we get 4 bytes of memory associated with each thread.
As a result we set the size of each element to 128 bytes and each thread in a Warp manages a single word or 4 bytes of the element.
Coincidentally an unsigned integer is 4 bytes wide on 64-bit systems, thus each thread accesses its memory in the form of an unsinged integer.
Using unsigned integers also has the advantage of allowing for simple bitwise operations, which are required since we store individual bits in the 4 bytes of memory instead of numerical values for most of the words in each element.
Numerical values are only stored in the base and next words of the element.
Each sparse bitvector element has the following layout.
\begin{center}
    \definecolor{lightcyan}{rgb}{0.84,1,1}
    \definecolor{lightgreen}{rgb}{0.64,1,0.71}
    \definecolor{lightred}{rgb}{1,0.7,0.71}
    \begin{bytefield}[bitheight=\widthof{~Base~},
            boxformatting={\centering\small},rightcurly=., rightcurlyspace=0pt, bitwidth=11pt]{32}
        \bitheader{0-31} \\
        \begin{rightwordgroup}{Words}
            \bitbox{29}[bgcolor=lightred]{Data} &
            \bitbox{1}[bgcolor=lightcyan]{\rotatebox{90}{Base}} &
            \bitbox{2}[bgcolor=lightgreen]{\rotatebox{90}{Next}}
        \end{rightwordgroup}
    \end{bytefield}
\end{center}
\subsubsection{64-bit Addresses}
This sparse bitvector layout is inspired by the layout put presented in \cite{mendez2012gpu}.
The notable difference being a switch from 32-bit to 64-bit addresses.
This is a requirement for using an address space larger than 16GiB, since we are addressing 4-byte wide words.
While this reduces the data density of each bitvector by 4 bytes, the theoretical address space of 64 exbibytes, or $2^66$ bytes, is worth the tradeoff as it eliminates any practical upper limits on memory allocation.
\subsubsection{Inserting new Bitvectors}
If our algorithm ever reaches a point where we need to add a new edge into an element with a mismatched base, we are required to create a new element with the correct base and append it to the likned list of bitvectors by referencing the correct memory address in the next field.
Since we are operating on multiple nodes concurrently, we need to keep track of the already occupied memory for each of the memory partitions. This is achieved by utilizing some of the counters from the CUDA initialization.
These are incremented via atomic CUDA instructions each time a new element is created.
Using atomic operations allows for a synchronized state of used memory across the entire grid and prevents Warps from overwriting already used memory.

\subsection{Edge Insertion}
With the concept of sparse bitvectors established and required variables initialized, the PTAGPU algorithm can begin inserting the linear in-memory representation of the constraint graph derived from SVF, see \autoref{sec:integsvf}, into sparse bitvectors residing on unified memory.
The core idea is to handle all edge labels concurrently in individual streams by invoking an edge insertion kernel for each type of edge.
Inside each kernel each source node is processed by a single Warp which inserts the outgoing edges into the correct memory location of the correct element inside the sparse bitvector for the given source node.
In order to efficiently distribute the work across all available Warps in all kernels, some pre-processing is needed before each kernel is launched.
Given a set of edges of the same type to be processed by a single kernel, for example all store edges in the constraint graph, the pre-processing steps are illustrated in \autoref{fig:edge-insert}.
\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{img/edge-insertion.png}
    \caption[Diagram for Edge Insertion]{Preprocessing of edges during edge insertion.}
    \label{fig:edge-insert}
\end{figure}
The pre-processing steps are made up of four steps, 1) the linear in-memory representation of the constraint graph is copied into device memory via \verb|cuda::memcpy| 2) all edge tuples $(src,dst)$ are sorted with $src$ as the key, 3) the algorithm iterates through all $(src,dst)$ pairs and eliminates all duplicates, 4) the algorithm iterates through all $(src,dst)$ pairs and saves the index each time src changes in a third offset array.
Since we are operating on linear memory, we can utilize the CUDA thrust library to accelerate these operations on the GPU.
Step 2 can be performed by the \verb|thrust::sort| algorithm and step 3 and 4 can be performed at the same time by \verb|thrust::unique_by_key_copy|.
One small but important detail is that before we start the pre-processing we insert a pair of \verb|UINT_MAX| values into the array for the source indices and into the array for the destination indices.
Having one additional pair assures that step 3 writes the end index for the last actual $(src,dst)$ pair into the offset array.

After the pre-processing is done, all three arrays are fed into an insertion kernel that distributes all source nodes among the available Warps, which then insert the destinations indices into the elements of the sparse bitvector.
Insertion into the element first calculates the correct base and then sets the correct bit to one if an element already exists, else a new element is first created.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}[squarednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm}]
            \def\step{0.3}
            \node[align=left,font = {\scriptsize}] at (20*\step,9*\step) {Example insertion of an edge pair $(3,66)$.\\This is achieved by finding the correct element in the sparse\\bitvector for node index 3 and setting the correct bit to 1 for node 66.\\\\1) Calculate $base = 66/(29*32) = 0$; $word = (66\%(29*32))/32 = 2$; $bit = 66\%32 = 2$\\2) Find the \verb|head element| for node 3 by using the getIndex function from \autoref{lst:getindex}.};
            \draw [decorate,decoration = {calligraphic brace,aspect=0.28,amplitude=10pt},line width=0.5mm] (0*\step,3*\step) --  (32*\step,3*\step);
            \foreach \r in {0,2,...,31} \node at (\r*\step+0.5*\step,1.5*\step) {\tiny\r};
            \draw[step=\step] (0,0) grid (32*\step,1*\step);
            \node at (29*\step+0.5*\step,0.5*\step) {\small U};\node at (30*\step+0.5*\step,0.5*\step) {\small U};\node at (31*\step+0.5*\step,0.5*\step) {\small U};
            \node[draw,align=left,font = {\tiny}] at (40*\step,0.5*\step) {Head element in the sparse\\bitvector for node 3.};
            
            \draw (2*\step,0) -- (0,-3*\step);\draw (3*\step,0) -- (32*\step,-3*\step);
            
            \draw[step=\step] (0,-3*\step) grid (32*\step,-4*\step);
            \foreach \r in {0,...,31} \node at (\r*\step+0.5*\step,-3.5*\step) {\small1};
            \foreach \r in {0,2,...,31} \node at (\r*\step+0.5*\step,-4.5*\step) {\tiny\r};
            \node[draw,align=left,font = {\tiny}] at (40*\step,-3.5*\step) {UINT bit representation of\\2nd index in element.\\Initially all bits are set to 1.};
            \node[align=left,font = {\scriptsize}] at (19*\step,-8.5*\step) {Since the base at index 29 in the head element is uninitialized (all bits set to 1), \\we can overwrite the base with the base we calculated for node 66 in step 1.\\Additionally we set the second bit in the second word to 1.};
            
            \foreach \r in {0,2,...,31} \node at (\r*\step+0.5*\step,-12.5*\step) {\tiny\r};
            \draw[step=\step] (0,-13*\step) grid (32*\step,-14*\step);
            \node at (29*\step+0.5*\step,-13.5*\step) {\small0};\node at (30*\step+0.5*\step,-13.5*\step) {\small U};\node at (31*\step+0.5*\step,-13.5*\step) {\small U};
            \draw (2*\step,-14*\step) -- (0,-17*\step);\draw (3*\step,-14*\step) -- (32*\step,-17*\step);
            \draw[step=\step] (0,-17*\step) grid (32*\step,-18*\step);
            \foreach \r in {0,...,1} \node at (\r*\step+0.5*\step,-17.5*\step) {\small0};\node at (2*\step+0.5*\step,-17.5*\step) {\small1};\foreach \r in {3,...,31} \node at (\r*\step+0.5*\step,-17.5*\step) {\small0};
            \foreach \r in {0,2,...,31} \node at (\r*\step+0.5*\step,-18.5*\step) {\tiny\r};
            \node[draw,align=left,font = {\tiny}] at (40*\step,-15.5*\step) {After inserting the edge $(3,66)$\\into the sparse bitvector.\\Notice that the base is set\\ to 0 and both words for the next\\ index are still set to \verb|UINT_MAX|\\marked by a U.};
            \draw[-{Stealth[red]}] (2.5*\step,-15*\step) -- (2.5*\step,-16.5*\step);
        \end{tikzpicture}
    \end{center}
    \caption{An example procedure for inserting a single edge into a sparse bitvector.}
    \label{fig:edgeinsert}
\end{figure}
The previous example in \autoref{fig:edgeinsert} showcases the process for inserting a single edge into an empty sparse bitvector.
Later insertions always check whether a base is already defined in each element of the sparse bitvector and insert the edges accordingly.
The base of subsequent elements in the sparse bitvectors is always sorted in ascending order for efficient random access. To maintain the order, whenever new elements are created, for example if a given base is not yet present in a sparse bitvector, elements might have to be shifted around.
In practice these insertions are executed for each source node in parallel by multiple Warps on the GPU.

\subsection{Concurrent Graph Rewriting}
As soon as all required edges from the constraint graph have been inserted into the correct sparse bitvectors, the Solver part of PTAGPU needs to apply the Andersen constraints from \autoref{tab:cfl-ander2} on the graph in order to calculate the correct points-to sets.
As with all calculations on the GPU we need to explicitly take care of concurrency.
Similar to edge insertion the ideal method of distributing work on the GPU is to assign units of work to individual Warps with minimal thread divergence.
When applying the production rules introduced in \autoref{tab:cfl-ander2} we can achieve this by simply using a worklist to spread out all node indices from the constraint graph across all available Warps.
This way each Warp handles the outgoing rewrite rules for a single node.
Furthermore this removes any need for specialized scheduling, because in the case that one Warp takes comparatively more time than others, the other Warps simply request new node indeces to work on.
The inherent synchronization of the worklist already takes care of optimally distributing the work.

Unfortunately we cannot simply use the production rules from the context free grammar approach in \autoref{tab:cfl-ander2}. The reason being that these rules can not be mapped onto strictly unidirectional paths in the graph.
As an example, consider the copy production rule $P\rightarrow \bar{C}P$. If we interpret this production rule in the context of graph rewriting, we get that for every node v with an outgoing copy edge to node x and an outgoing points-to edge to w, we need to add a new points-to edge from x to w.
The problem arises when multiple Warps operating on outgoing edges of different nodes, simultaneously try to mutate anther node's outgoing edges.
See \autoref{fig:problem-rewrite} a) for an illustration of this problem from \cite{mendez2012gpu}.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{img/rewriting-concurrent.png}
    \caption[Diagram for Concurrent Graph Rewriting]{Taken from \cite{mendez2012gpu}, illustrates the problem of cross node mutation when applying unmodified Andersen constraints.}
    \label{fig:problem-rewrite}
\end{figure}
A key insight from \cite{mendez2012gpu} was that, "Parallel execution of the rewrite rules \dots requires synchronization in the graph data structure.".
The intuitive solution to this problem is shown in \autoref{fig:problem-rewrite} b). For most constraints, simply inverting the edges is enough to allow for concurrent addition of new edges into the sparse bitvectors without mutatung other node's outgoing edges.
The resulting graph rewriting rules from \cite{mendez2012gpu} are listed in \autoref{tab:rewrite-rules}.
\begin{table}
    \begin{center}
        \caption{Final unidirectional, partly inverted Graph Rewriting Rules proposed by \cite{mendez2012gpu}.\\Field-sensitive rules are omitted, since they are exclusively handled on the CPU, see \autoref{sec:cpugpu}.}
        \label{tab:rewrite-rules}
        \begin{tabular}{l|l|l}
            \hline                                                                                                                             \\
            \textbf{Statement} & \textbf{Name} & \textbf{Rewrite Rule}                                                                         \\
            \hline                                                                                                                             \\
            $x = y$            & copy          & $x \xrightarrow{c^{-1}} y \xrightarrow{p} z \mathrel{\leadsto} x \xrightarrow{p} z$           \\
            $x = *y$           & load          & $x \xrightarrow{l^{-1}} y \xrightarrow{p} z \mathrel{\leadsto} x \xrightarrow{c^{-1}} z$      \\
            $*x = y$           & store         & $x \xrightarrow{p^{-1}} y \xrightarrow{s^{-1}} z \mathrel{\leadsto} x \xrightarrow{c^{-1}} z$ \\
        \end{tabular}
    \end{center}
\end{table}
One remaining problem is, that the store rewrite rule currently require inverse point-to edge information which, if additionally stored, would greatly increase the required memory by keeping track of outgoing and incoming points-to edges at the same time for each node.
The proposed solution to this problem from \cite{mendez2012gpu} is to apply the store rewrite rules in two steps. First all node pairs $(x,y)$ are collected such that y has outgoing store edges and points to x. Second all pairs with matching x are assigned to the same Warp and the resulting inverse copy edges are added to the sparse bitvectors of x, denoting the destinations of the outgoing inverse store edges from node y.
Using this method requires one additional step compared to the copy and load rules but this step can be processed very efficiently with thrust library calls, similar to the edge insertion routine.
With this two step procedure in place, we can preserve the property that each Warp only appends outgoing edges to the sparse bitvector of it's currently assigned node and refrain from explicit synchronization.
\subsubsection{Rule Application Algorithm}\label{sec:main-algo}
With both data structures and graph rewrite rules in place and optimized for operation on the GPU, the main algorithm of PTAGPU can be launched.
The pseudocode for the most important procedures of PTAGPU is available, the main loop can be found in \autoref{alg:ptagpu-main}.
The first step is to execute the initialization steps from \autoref{sec:init}.
Since we want to apply all rewrite rules of the Andersen style analysis until no more changes are applied, we perform the application of the rules in a while loop that breaks if no changes are detected in after one iteration finishes.
We can observe the changes by counting the number of points-to edges present in all sparse bitvectors.
While it might seem inefficient to iterate through all elements in each iteration of the loop, this can be done very efficiently, since we employ massively parallel GPU kernels.
If changes to the points-to memory region have been detected, the algorithm continues with updating the newPts, currentPts and oldPts memory.
This serves the purpose of preventing redundant work, by only applying rewrite rules on the points-to edges that have been added since the last iteration.
This process is called a delta-update or diffpoints-update, the specifics of this optimization are presented in \autoref{sec:diffpts} and the pseudocode is available in \autoref{alg:ptagpu-updatepts}.
After delta-updates are done, all points-to edges that are new since the last iteration are present in the currentPts memory region.
Before launching the first kernel for the rewrite rules, we can launch the CPU portion of our analysis asynchronously.
In practice this is realized by using C++ asynchronous futures.
See \autoref{sec:cpugpu} for the details of this forked CPU portion of the main loop.
\begin{algorithm}
    \caption{Main Algorithm of PTAGPU}\label{alg:ptagpu-main}
    \begin{algorithmic}
        \State \texttt{initialization steps}
        \While{\texttt{new points-to edges written into memory} $\wedge$ not done}
        \State \texttt{run updatepts kernel} \algorithmiccomment{see Algorithm 3}
        \State \texttt{launch CPU code asynchronously} \algorithmiccomment{see Algorithm 6}
        \State \texttt{run copy/load/storecollect kernel} \algorithmiccomment{see Algorithm 4}
        \State \texttt{sort $(x,y)$ pairs for the store constraints}
        \State \texttt{split pairs by unique x component}
        \State \texttt{run store kernel} \algorithmiccomment{see Algorithm 5}
        \State \texttt{await asynchronous CPU execution}
        \State \texttt{insert new edges found by CPU code}
        \State \texttt{report memory statistics and new edge count} \algorithmiccomment{optional}
        \EndWhile
        \State \texttt{free memory and pass results back to SVF}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Update Points Kernel}\label{alg:ptagpu-updatepts}
    \begin{algorithmic}
        \For{\texttt{each node i in constraint graph}}
        \State \texttt{traverse nextPoints sparse bitvector for node i}
        \State \texttt{collect all set bits}
        \State \texttt{compute $\Delta \mathcal{P} = nextPoints\oplus oldPoints$} \algorithmiccomment{find all new bits via xor}
        \State \texttt{update $currentPoints = \Delta \mathcal{P}$}
        \If{$\Delta \mathcal{P} \neq \emptyset$}
        \State \texttt{set done to $\mathbf{false}$}
        \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Copy Load Storecollect Kernel}\label{alg:ptagpu-copyload}
    \begin{algorithmic}
        \State \texttt{smem $\leftarrow$ allocate 256 bytes of shared memory per Warp}
        \State \texttt{upperlimit $\leftarrow$ totalNodeCounter}
        \State \texttt{src $\leftarrow$ incrementWorklist()}
        \While{\texttt{src < upperlimit}}
        \State \texttt{applyRewriteRule<Copy>(src,smem)} \algorithmiccomment{see Algorithm 7}
        \State \texttt{applyRewriteRule<Load>(src,smem)}
        \State \texttt{src $\leftarrow$ incrementWorklist()}
        \EndWhile
        \State \texttt{upperlimit $\leftarrow$ totalStoreCounter}
        \State \texttt{src $\leftarrow$ incrementStoreWorklist()}
        \While{\texttt{src < upperlimit}}
        \State \texttt{applyRewriteRule<CollectStore>(src,smem)} \algorithmiccomment{collect store/pts pairs for store kernel}
        \State \texttt{src $\leftarrow$ incrementStoreWorklist()}
        \EndWhile
        \State \texttt{reset worklists}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Store Kernel}\label{alg:ptagpu-store}
    \begin{algorithmic}
        \State \texttt{smem $\leftarrow$ allocate 256 bytes of shared memory per Warp}
        \For{\texttt{some x of collected $(x,y)$ pts/$\text{store}^{-1}$ pairs}}
        \For{\texttt{each unique y of collected $(x,y)$ pts/$\text{store}^{-1}$ pairs}}
        \State \texttt{append y to smem}
        \EndFor
        \State \texttt{mergeBitvectors(x,smem)} \algorithmiccomment{see Algorithm 9}
        \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\texttt{CPU async} procedure}\label{alg:ptagpu-cpu}
    \begin{algorithmic}
        \State \texttt{typedef pair<vector<uint>,vector<uint>> edgeSet}
        \Procedure{\texttt{asyncCPU}}{\texttt{edgeSet *ptsSet, edgeSet *copySet}}
        \State \texttt{\#pragma omp parallel for num\_threads(16)}
        \For{\texttt{each GEP edge in constraint graph}}
        \State \texttt{dstPTS $\leftarrow \emptyset$ }
        \State \texttt{srcPTS $\leftarrow$ collectBitvectorTargets<PTS>(edge.src)}
        \For{\texttt{id in srcPTS}}
        \State \texttt{fieldOffsetNode $\leftarrow$ consCG->getGepObjVar(id, edge.offset)}
        \State \texttt{\#pragma omp critical} \algorithmiccomment{only executed by one thread at a time}
        \State \texttt{append fieldOffsetNode to dstPTS}
        \EndFor
        \State \texttt{\#pragma omp critical}
        \For{\texttt{id in dstPTS}}
        \State \texttt{addPts(edge.dst,id)}
        \EndFor
        \EndFor
        \State \texttt{updateCallGraph(getIndirectCallsites())} \algorithmiccomment{these are SVF superclass methods}
        \State \texttt{// some of getIndirectCallsites's internal function calls are}
        \State \texttt{// overwritten to use unified memory instead of SVF data structures}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\texttt{template<Type> applyRewriteRule} procedure\\
        \begin{minipage}[t]{0.1\textwidth}
            \textbf{Input:}
        \end{minipage}
        \begin{minipage}[t]{0.8\textwidth}
            src: currently processing node\\
            smem: shared memory
        \end{minipage}
    }\label{alg:ptagpu-applyrewrite}
    \begin{algorithmic}
        \State \texttt{index $\leftarrow$ getIndex(src)}
        \Repeat
        \State \texttt{bits $\leftarrow$ memory[index + threadIdx.x]}
        \State \texttt{collectBitvectorTargets<Type>(src,bits,smem)} \algorithmiccomment{Algorithm 8}
        \State \texttt{index $\leftarrow$ nextBits}
        \Until{\texttt{index $\neq$ ULLONG\_MAX}}
        \If{\texttt{Type = StoreCollect}}
        \State \texttt{insert all pairs $(src,smem[:])$ into store map} \algorithmiccomment{later used in the store kernel}
        \Else
        \State \texttt{mergeBitvectors(src,smem)} \algorithmiccomment{Algorithm 9}
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\texttt{template<Type> collectBitvectorTargets} procedure\\
        \begin{minipage}[t]{0.1\textwidth}
            \textbf{Input:}
        \end{minipage}
        \begin{minipage}[t]{0.8\textwidth}
            to: currently processing node\\
            bits: of the head element in to's sparse bitvector\\
            smem: shared memory
        \end{minipage}
    }\label{alg:ptagpu-collectbv}
    \begin{algorithmic}
        \If{\texttt{bits $\neq$ 0}}
        \For{\texttt{i = 0\dots 31}}
        \If{\texttt{(1<<i) $\wedge$ bits}} \algorithmiccomment{check if bit is set}
        \State \texttt{target $\leftarrow$ base * (29*32) + threadIdx.x * 32 + i}
        \State \texttt{append target to smem} \algorithmiccomment{target represents the destination of the edge}
        \EndIf
        \EndFor
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\texttt{mergeBitvectors} procedure\\
        \begin{minipage}[t]{0.1\textwidth}
            \textbf{Input:}
        \end{minipage}
        \begin{minipage}[t]{0.8\textwidth}
            to: node that outgoing edges are merged into\\
            smem: array of nodes whose outgoing edges are to be merged with to
        \end{minipage}
    }\label{alg:ptagpu-mergebv}
    \begin{algorithmic}
        \For{\texttt{from $\leftarrow$ smem}}
        \State \texttt{toIndex $\leftarrow$ getIndex(to)} \algorithmiccomment{find the index of the head element}
        \State \texttt{fromIndex $\leftarrow$ getIndex(from)}
        \While{\texttt{next element $\neq$ ULLONG\_MAX}}
        
        \If{\texttt{fromBase = toBase}}
        \State \texttt{merge element at fromIndex into toIndex}
        \ElsIf{\texttt{fromBase < toBase}}
        \State \texttt{insert new element before toIndex} \algorithmiccomment{elements are sorted by base}
        \Else
        \If{\texttt{toIndex has next element}}
        \State \texttt{toIndex $\leftarrow$ toNext}
        \Else
        \State \texttt{create new element after toIndex and insert fromIndex}
        \EndIf
        \EndIf
        
        \State \texttt{toIndex $\leftarrow$ toNext} \algorithmiccomment{read next elements in sparse bitvectors}
        \State \texttt{fromIndex $\leftarrow$ fromNext}
        \EndWhile
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsubsection{Diffpoints Updates}\label{sec:diffpts}
delta updates
\subsection{Combining CPU and GPU}\label{sec:cpugpu}
\subsection{Integration with SVF}
alias method
\subsubsection{Parallelizing SVF}
\section{Experimental Results}
\subsection{Test Suite}
\subsection{Benchmark Suite}
\section{Evaluation}
